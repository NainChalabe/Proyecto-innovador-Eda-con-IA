{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "8XlJV5HEvmbR",
        "outputId": "c39f93ec-236f-4ab3-b273-0731328a16d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando datos...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1921ab3b0f4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-1921ab3b0f4c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data'\u001b[0m  \u001b[0;31m# Cambiar a la ruta de tu carpeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cargando datos...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datos cargados.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1921ab3b0f4c>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mdataframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twittersentimentanalysis.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data'"
          ]
        }
      ],
      "source": [
        "# Librerías necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Descargar recursos de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1️⃣ Exploración del Dataset (EDA)\n",
        "\n",
        "# Cargar el dataset\n",
        "file_path = 'twittersentimentanalysis.csv'  # Cambia esta ruta\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Inspección inicial\n",
        "print(\"Primeras filas del dataset:\")\n",
        "print(data.head())\n",
        "print(\"\\nDistribución de sentimientos:\")\n",
        "print(data['Sentiment'].value_counts())\n",
        "\n",
        "# Visualización de la distribución de sentimientos\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data, x='Sentiment', palette='viridis')\n",
        "plt.title('Distribución de Sentimientos')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('Cantidad')\n",
        "plt.show()\n",
        "\n",
        "# Generación de WordCloud\n",
        "def generar_wordcloud(sentimiento):\n",
        "    texto = \" \".join(data[data['Sentiment'] == sentimiento]['Tweet'].astype(str))\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Nube de Palabras para Sentimiento {sentimiento}')\n",
        "    plt.show()\n",
        "\n",
        "generar_wordcloud('positive')\n",
        "generar_wordcloud('negative')\n",
        "generar_wordcloud('neutral')\n",
        "\n",
        "# 2️⃣ Preprocesamiento y Entrenamiento del Modelo\n",
        "\n",
        "# Configuración del preprocesamiento\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(texto):\n",
        "    tokens = word_tokenize(texto.lower())\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['texto_procesado'] = data['Tweet'].apply(preprocess)\n",
        "\n",
        "# Vectorización y división de datos\n",
        "X = data['texto_procesado']\n",
        "y = data['Sentiment']\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_vectorizado = vectorizer.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorizado, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "modelo = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "# Evaluación del modelo\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 3️⃣ Guardar el modelo y el vectorizador\n",
        "with open('modelo.pkl', 'wb') as archivo_modelo:\n",
        "    pickle.dump(modelo, archivo_modelo)\n",
        "with open('vectorizador.pkl', 'wb') as archivo_vectorizador:\n",
        "    pickle.dump(vectorizer, archivo_vectorizador)\n",
        "\n",
        "print(\"\\n¡El modelo y el vectorizador han sido guardados exitosamente!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
